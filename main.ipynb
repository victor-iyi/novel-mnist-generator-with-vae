{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# <span style=\"color:brown\"> Variational Auto Encoder (VAE)\n",
    "\n",
    "### <span style=\"color:blue\"> by Victor I. Afolabi\n",
    "\n",
    "Autoencoders are a type of neural network that can be used to learn efficient codings of input data. Given some inputs, the network first applies a series of transformations that map the input data into a lower dimensional space. This part of the network is called the ***encoder***.\n",
    "\n",
    "Then, the network uses the encoded data to try and recreate the inputs. This part of the network is the ***decoder***. Using the encoder, we can compress data of the type that is understood by the network. However, autoencoders are rarely used for this purpose, as usually there exist hand-crafted algorithms (like jpg-compression) that are more efficient.\n",
    "\n",
    "Instead, autoencoders have repeatedly been applied to perform de-noising tasks. The encoder receives pictures that have been tampered with noise, and it learns how to reconstruct the original images.\n",
    "\n",
    "One such application for *autoencoders* is called the **variational autoencoder**. Using variational autoencoders, it’s not only possible to compress data — it’s also possible to generate new objects of the type the autoencoder has seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('datasets/MNIST', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "image_size = 28\n",
    "image_channel = 1\n",
    "image_size_flat = image_size * image_size * image_channel\n",
    "image_shape = [image_size, image_size, image_channel]\n",
    "\n",
    "# Network\n",
    "keep_prob = 0.8\n",
    "n_latent = 8\n",
    "decoder_units = int(32 * image_channel / 2)\n",
    "\n",
    "# Training\n",
    "learning_rate=1e-3\n",
    "batch_size = 24\n",
    "iterations = 10000\n",
    "log_step = 100\n",
    "viz_step = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model's placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, image_size_flat], name='X_placeholder')\n",
    "y = tf.placeholder(tf.float32, shape=[None, image_size_flat], name='y_placeholder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def leakyReLU(X, alpha=0.3):\n",
    "    return tf.maximum(X, tf.multiply(X, alpha))\n",
    "\n",
    "def conv2d(X, filters=64, kernel_size=4, strides=2, padding='SAME', activation=tf.nn.relu, dropout=True):\n",
    "    layer = tf.layers.conv2d(inputs=X, filters=filters, kernel_size=kernel_size, \n",
    "                            strides=strides, padding=padding, activation=activation)\n",
    "    if dropout:\n",
    "        layer = tf.nn.dropout(layer, keep_prob=keep_prob)\n",
    "    return layer\n",
    "\n",
    "def conv2d_transpose(X, filters=64, kernel_size=4, strides=2, padding='SAME', activation=tf.nn.relu, dropout=True):\n",
    "    layer = tf.layers.conv2d_transpose(inputs=X, filters=filters, kernel_size=kernel_size, \n",
    "                                       strides=strides, padding=padding, activation=activation)\n",
    "    if dropout:\n",
    "        layer = tf.nn.dropout(layer, keep_prob=keep_prob)\n",
    "    return layer\n",
    "\n",
    "def dense(X, units, activation=leakyReLU):\n",
    "    return tf.layers.dense(inputs=X, units=units, activation=activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The Encoder\n",
    "\n",
    "As our inputs are images, it’s most reasonable to apply some convolutional transformations to them. What’s most noteworthy is the fact that we are creating two vectors in our encoder, as the encoder is supposed to create objects following a Gaussian Distribution:\n",
    "\n",
    "* A vector of means\n",
    "* A vector of standard deviations\n",
    "\n",
    "You will see later how we *“force”* the encoder to make sure it really creates values following a Normal Distribution. The returned values that will be fed to the decoder are the z-values. We will need the mean and standard deviation of our distributions later, when computing losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def encoder(X):\n",
    "    with tf.variable_scope('encoder', reuse=None):\n",
    "        X = tf.reshape(X, shape=[-1, image_size, image_size, image_channel])\n",
    "        X = conv2d(X, activation=leakyReLU, dropout=True)\n",
    "        X = conv2d(X, activation=leakyReLU, dropout=True)\n",
    "        X = conv2d(X, strides=1, activation=leakyReLU, dropout=True)\n",
    "        X = tf.contrib.layers.flatten(X)\n",
    "        mean = tf.layers.dense(X, units=n_latent)\n",
    "        stddev = 0.5 * tf.layers.dense(X, units=n_latent) # 0.5 * mean\n",
    "        noise = tf.random_normal(tf.stack([tf.shape(X)[0], n_latent]))\n",
    "        print(noise)\n",
    "        z = mean + tf.multiply(noise, tf.exp(stddev))\n",
    "        return z, mean, stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The Decoder\n",
    "\n",
    "The decoder does not care about whether the input values are sampled from some specific distribution that has been defined by us. It simply will try to reconstruct the input images. To this end, we use a series of *transpose convolutions*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decoder(z):\n",
    "    with tf.variable_scope('decoder', reuse=None):\n",
    "        X = dense(z, units=decoder_units, activation=leakyReLU)\n",
    "        X = dense(z, units=decoder_units*2, activation=leakyReLU)\n",
    "        shape = X.get_shape()[1].value // 2\n",
    "        reshape_dim = [-1, shape, shape, image_channel]\n",
    "        X = tf.reshape(X, reshape_dim)\n",
    "        X = conv2d_transpose(X, dropout=True)\n",
    "        X = conv2d_transpose(X, strides=1, dropout=True)\n",
    "        X = conv2d_transpose(X, strides=1, dropout=False)\n",
    "        X = tf.contrib.layers.flatten(X)\n",
    "        X = dense(X, units=image_size_flat, activation=tf.nn.sigmoid)\n",
    "        img = tf.reshape(X, [-1, *image_shape])\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z, mean, stddev = encoder(X)\n",
    "img = decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "img_reshape = tf.reshape(img, [-1, image_size_flat])\n",
    "\n",
    "img_loss = tf.reduce_sum(tf.squared_difference(img_reshape, y), axis=1)\n",
    "latent_loss = -0.5 * tf.reduce_sum(1.0 + 2.0 * stddev - tf.square(mean) - tf.exp(2.0*stddev), axis=1)\n",
    "loss = tf.reduce_mean(img_loss + latent_loss)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tensorflow's `Session`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tensorboard_path = 'tensorboard/'\n",
    "save_path = 'models/'\n",
    "logdir = os.path.join(tensorboard_path, 'log')\n",
    "pretrained = os.path.join(save_path, 'model.ckpt')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "writer = tf.summary.FileWriter(logdir=logdir, graph=sess.graph)\n",
    "\n",
    "if tf.gfile.Exists(save_path):\n",
    "    if len(os.listdir(save_path)) > 1:\n",
    "        saver.restore(sess=sess, save_path=save_path)\n",
    "else:\n",
    "    tf.gfile.MakeDirs(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_start = dt.datetime.now()\n",
    "for i in range(iterations):\n",
    "    batch = data.train.next_batch(batch_size=batch_size)[0]\n",
    "    feed_dict = {X: batch, y: batch}\n",
    "    sess.run(train_step, feed_dict=feed_dict)\n",
    "    if i % log_step == 0:\n",
    "        _loss, _img_loss, _latent_loss, _mean, _stddev = sess.run([loss, img_loss, latent_loss, mean, stddev], feed_dict=feed_dict)\n",
    "        sys.stdout.write('\\rLoss={:.2f}\\timg_loss = {:.2f}\\tlatent_loss = {:.2f}\\tmean = {.2f}\\tstddev = {:.2f}\\tTime taken = {}'.format(\n",
    "            _loss, _img_loss, _latent_loss, _mean, _stddev, dt.datetime.now() - start_time\n",
    "        ))\n",
    "    if i % viz_step == 0:\n",
    "        _reconstruct = sess.run(img, feed_dict=feed_dict)\n",
    "        plt.imshow(np.reshape(batch[0], image_shape), cmap='Greys')\n",
    "        plt.imshow(_reconstruct[0], cmap='Greys')\n",
    "        plt.show()\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
